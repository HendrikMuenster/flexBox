
\documentclass[final,leqno,onefignum,onetabnum]{article}

\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{amsfonts}

  \usepackage[caption=false]{subfig}
  \usepackage{graphicx}
  \usepackage{todonotes}
  \usepackage{tikz,tikz-cd,pgf}
\usepackage{listings}
\usepackage[numbered]{mcode} %for matlab code
  
  \usepackage{algorithm}
  \usepackage{algpseudocode}
  
  \usepackage{pgfplots}
  \pgfplotsset{compat=1.12}
  
  \usepackage{multirow}
  \newcommand{\R}{\mathbb{R}}
  \newcommand{\N}[1]{\mathbb{N}^{#1}}
  \newcommand{\1}[1]{\mathds{1}_{#1}}
  \newcommand{\st}{\qquad\text{s. t.}\qquad}

  \renewcommand{\liminf}[1]{\underset{#1}{\lim\inf}\;}
  \renewcommand{\limsup}[1]{\underset{#1}{\lim\sup}\;}
  
  \newcommand{\CX}{\mathcal{X}}
  \newcommand{\CY}{\mathcal{Y}}
  \newcommand{\CZ}{\mathcal{Z}}
  \newcommand{\dif}{\mathrm{d}}
  
  \DeclareMathOperator*{\argmin}{\arg \min}%
  \DeclareMathOperator*{\argmax}{\arg \max}%
  \newcommand*\cpp{C\kern-0.2ex\raisebox{0.4ex}{\scalebox{0.8}{+\kern-0.4ex+}}}
  

  %For pseudocode
 \newcommand*\Let[2]{\State #1 $\gets$ #2}
  
\title{A \textbf{\underline{Flex}}ible Primal-Dual Tool\textbf{\underline{Box}} \\\large Technical Report}
\author{Hendrik Dirks}


\begin{document}
	
\maketitle

\begin{abstract}
	Abstract
\end{abstract}

\section{Introduction}
Many variational problems can be written in the form
\begin{align}
	\argmin_{u} G(u) + F(Ku),
	%\argmin_{u=u_1,\ldots,u_n} \sum_{i=1}^{m_1} G_i(u) + \sum_{j=1}^{m_2} F_j(K_ju),
	\label{generalPrimalFormulation}
\end{align}
which is generally denoted as the \textit{primal} formulation of the minimization problem. It can be shown that minimizing \eqref{generalPrimalFormulation} is equivalent \todo{requirements} to minimizing the \textit{primal-dual} or \textit{saddle-point} formulation
\begin{align}
	\argmin_{u}\argmax_{p} G(u) + \langle y,Ku\rangle - F^*(y).
\label{generalPrimalDualFormulation}
\end{align}
Here $F^*$ refers to the convex conjugate of $F$. It the recent years, algorithms like ADMM or primal-dual \todo{ref} for efficiently solving these saddle-point problems have become very popular. \textbf{FlexBox} makes use the latter, which can be sketched up as follows:\\
For $\tau,\sigma>0$, a pair $(v^0,y^0)\in\mathcal{X}\times\mathcal{Y}$ and initial value $\bar{v}^0=0$ we obtain the following iterative scheme:
\begin{align}
	y^{k+1} &= prox_{\sigma F^*}(y^k+\sigma K\hat{x}^k)\\
	x^{k+1} &=  prox_{\tau G}(x^k-\tau K^* y^{k+1})\\
	\hat{x}^{k+1} & = 2 x^{k+1} -x^k
	\label{pdAlgorithm}
\end{align}
Here, $prox_{\tau G}$ denotes the \textit{proximal} or \textit{resolvent} operator
\begin{align*}
	prox_{\tau G}(y) = \left( I+\tau \partial G\right)^{-1}(y) := \argmin_v \left\{ \frac{\left\| v-y \right\|_2^2}{2}+\tau G(v) \right\} ,
\end{align*}
which can be interpreted as a compromise between minimizing $G$ and being close to the input argument $y$. The efficiency of primal-dual algorithms relies on the fact that the $prox$ problems are easy to compute.
\subsection{Contibution}
Since primal-dual algorithms have been extensively applied to all classes of convex optimization problems we found that people are spending a lot of effort on calculating convex conjugates or solutions for prox-problems again and again for similar problems. Let us consider for example the isotropic total variation $\|\nabla u\|_{1,2}$, where the convex conjugate is an indicator function of the L$^\infty$-ball and the solution of the prox-problem is a point-wise projection onto L$^2$-balls. These results hold not only for $K=\nabla$, but for arbitrary operators. \textbf{FlexBox} makes use of this generalization and simply works on the level of terms in the primal problem. After adding a certain term, \textbf{FlexBox} automatically decouples operators, creates dual variables and calculates step-sizes (e.g. $\tau, \sigma$). \textbf{FlexBox} already contains a variety of data-fidelity and regularization terms\todo{REF zu Liste an Termen}, but is also with to user-defined operators. Moreover, the class-based structure allows easy extension and creation custom terms.\\
\textbf{FlexBox} is mainly written in MALTAB, but has an optional \cpp module to improve runtime. This module can be compiled and is afterwards automatically used via a mex-interface. The \cpp module can also be used without Matlab, but does not yet have the full variety of terms included.

\section{Architecture and Features of FlexBox}
\paragraph{Design:} 
\textbf{FlexBox} is designed as one main class which holds a list of functional terms that are either \textit{primal} or \textit{dual}. Moreover, the main object holds the data of all primal and dual variables $x_i$ and $y_i$. Primal and dual terms always correspond to at least primal variable, whereas dual terms also correspond at least one dual variable. \textbf{FlexBox} automatically creates dual variables once a dual term is added. In the primal-dual algorithm \eqref{pdAlgorithm}, applications of the operator can be decoupled by defining $\tilde{y} := y^k+\sigma K\hat{x}^k$ and $\tilde{x} := x^k-\tau K^* y^{k+1}$. 

Both, primal and dual terms, contain a corresponding prox-method  

\section{Examples}

\subsection{Rudin-Osher-Fatemi}
The Rudin-Osher-Fatemi model \todo{REF} has very popular applications in image denoising. The primal-formulation reads
\begin{align}
	\argmin_u \frac{1}{2}\|u-f\|_2^2 + \alpha \|\nabla u\|_{1,2},
\end{align}
where the first part fits the unknown $u$ to the given input image $f$ and the second part refers to the isotropic total variation, which penalizes the total number jumps in the solution. Minimizing this problem with FlexBox can be done with the following lines of code

\begin{lstlisting} 
%Begin: Code example
main = flexbox;

numberU = main.addPrimalVar(size(f));

main.addTerm(L2dataTerm(0.5,f),numberU);
main.addTerm(L1gradientIso(0.08,size(f)),numberU);

main.runAlgorithm;

result = main.getPrimal(numberU);
%End: Code example
\end{lstlisting}
Let us begin in line $2$, which initializes a \textbf{FlexBox} object and saves it to the variable $main$. Line 4 then adds the primal objective variable $u$ which has the same size as the input image $f$. The toolbox returns the internal number of this primal variable, which is saved in $numberU$. \\
In line 6 and 7, the L$^2$-data-term with weight $0.5$ and corresponding image $f$ is added, moreover the isotropic TV-term with weight $0.08$ is pushed into the framework. Please mention that the function $addTerm$ always requires a functional part and the internal number of the corresponding primal variable. \\
The function call in line $9$ finally starts the calculation and once this is finished we transfer the solution into the variable $result$ in line $11$. 

\subsection{Optical Flow}
Estimating the motion between two consecutive images $f_1$ and $f_2$ based on the displacement of intensities in both images is called optical-flow estimation. The unknown velocity field $\boldsymbol{v}$ is usually connected to the image by the brightness-constancy-assumption $f_2(x+\boldsymbol{v})-f_1(x)=0$. This formulation is non-linear in terms of $\boldsymbol{v}$ and therefore linearized. A corresponding variational problem incorporating total variation regularization can be written as
\begin{align}
	\argmin_{\boldsymbol{v}=(v_1,v_2)} \frac{1}{2}\|f_2-f_1 + \nabla f_2\cdot\boldsymbol{v}\|_1 + \alpha_1 \|\nabla v_1\|_{1,2} + \alpha_2\|\nabla v_2\|_{1,2},
\end{align}
Solving this problem with \textbf{FlexBox} can be done in a similar manner as for the ROF example:
\begin{lstlisting} 
%Begin: Code example
main = flexbox;

numberV1 = main.addPrimalVar(size(f1));
numberV2 = main.addPrimalVar(size(f2));

%add optical flow data term
main.addTerm(L1opticalFlowTerm(1,f1,f2),[numberV1,numberV2]);

%add regularizers - one for each component
main.addTerm(L1gradientIso(0.05,size(f1)),numberV1);
main.addTerm(L1gradientIso(0.05,size(f1)),numberV2);

main.runAlgorithm;

resultV1 = main.getPrimal(numberV1);
resultV2 = main.getPrimal(numberV2);
%End: Code example
\end{lstlisting}
In lines 4 and 5 primal variables for both components of the velocity fields are added. Afterwards, in lines 8, 11 and 12 the data term and regularizers for both components are inserted. Please note that the optical flow term now refers to two primal variables written as the vector $[numberV1,numberV2]$. Afterwards, the algorithm is started and both results are retrieved.







\subsection{Labeling}

\section{How-To}

\section{Arbitrary Operators}
\textbf{Classes}: \textit{L1operatorIso, L1operatorAniso, L2operator, frobeniusOperator}\\
Adding an arbitrary operator in some norm is simple. Let us assume we some term $\alpha \|Ku\|_{1,2}$ with 
\begin{align*}
	K=\begin{pmatrix}K_1&K_2\\K_3&K_4\end{pmatrix}, u=\begin{pmatrix}u_1\\u_2\end{pmatrix},
\end{align*}
where the norm $\|\cdot\|_{1,2}$ refers to the isotropic L1-norm. We can insert this term into \textbf{FlexBox} by calling
\begin{lstlisting} 
%Begin: Code example
main.addTerm(L1operatorIso(alpha,2,{K_1,K_2,K_3,K_4}),[numberU_1,numberU_2]);
%End: Code example
\end{lstlisting}
The operator $K$ has to be specified row-wise in a cell-array. The second argument $2$ tells the toolbox that every two elements in the cell-array correspond to one row. Please note that empty blocks in $K$ have to be specified as empty sparse matrices.

\begin{appendix}
\section{Mathematical Setup}	
	
\section{Prox Calculation}

\subsection{Frobenius-Norm}
Minimization problem
\begin{align}
	\min_u \alpha\|Ku\|_F	
\end{align}
Decuopling the operator $K$ reads
\begin{align}
	F(\tilde{u}) = \alpha\|\tilde{u}\|_F
\end{align}
Calculating convex conjugate (Frobenius-norm is self-adjoint)
\begin{align}
	F^*(y) = \delta_{\{ \|y/\alpha\|_{F^*} \leq 1 \}} = \delta_{\{ \|y/\alpha\|_{F} \leq 1 \}}
\end{align}
Prox reads:
\begin{align}
	prox_{\sigma F^*}(\tilde{y}) = \min_y \|y-\tilde{y}\|_2^2 + \delta_{\{ \|y/\alpha\|_{F} \leq 1 \}}	
\end{align}
Some calculation yields the compact solution
\begin{align}
	prox_{\sigma F^*}(\tilde{y}) = \frac{\tilde{y}}{\max\{1, \|\tilde{y}/\alpha\|_{F} \}}
\end{align}


\subsection{Kullback-Divergence}
\todo{Calculate with factor alpha}
Minimization problem
\begin{align}
\min_u Ku-f+f\log\frac{f}{Ku} + \delta_{\{ \cdot\geq 0 \}}(Ku)	
\end{align}
Decuopling the operator reads
\begin{align}
	F(\tilde{u}) = \tilde{u}-f+f\log\frac{f}{\tilde{u}} + \delta_{\{ \cdot\geq 0 \}}(\tilde{u})	
\end{align}
Calculating convex conjugate
\begin{align}
F^*(y) = -f\log(\boldsymbol{1}-y) + \delta_{\{ y\geq 0 \}}(\boldsymbol{1}-y)	
\end{align}
Prox reads:
\begin{align}
prox_{\sigma F^*}(\tilde{y}) = \min_y \frac{1}{2}\|y-\tilde{y}\|_2^2 -\sigma f\log(\boldsymbol{1}-y) + \delta_{\{ y\geq 0 \}}(\boldsymbol{1}-y)
\end{align}
Some calculation yields the compact solution
\begin{align}
prox_{\sigma F^*}(\tilde{y}) = \frac{1}{2}\left( \boldsymbol{1} + \tilde{y} - \sqrt{ (\tilde{y}-\boldsymbol{1})^2 + 4\sigma f }\right) 
\end{align}

\subsection{Inner Products}
\begin{align}
	\min_u \alpha\langle Ku,b\rangle
\end{align}
Decoupling the operator reads
\begin{align}
	F(\tilde{u}) = \langle\tilde{u},\alpha b\rangle
\end{align}
Calculating convex conjugate
\begin{align}
	F^*(y) = \sup_{\tilde{u}} \langle\tilde{u},y\rangle -\langle\tilde{u},\alpha b\rangle = \sup_{\tilde{u}} \langle\tilde{u},y - \alpha b\rangle = \begin{cases}0&\text{for } y=\alpha b\\\infty&\text{else}\end{cases}
\end{align}
Prox reads:
\begin{align}
prox_{\sigma F^*}(\tilde{y}) = \min_y \frac{1}{2}\|y-\tilde{y}\|_2^2 +\sigma \delta_{\{ y = \alpha b \}}
\end{align}
Compact solution
\begin{align}
prox_{\sigma F^*}(\tilde{y}) = \alpha b
\end{align}


\end{appendix}
	
\listoftodos

%\bibliographystyle{plain}
%\bibliography{references}
	
\end{document}