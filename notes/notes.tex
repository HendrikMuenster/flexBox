
\documentclass[final,leqno,onefignum,onetabnum]{article}

\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{amsfonts}

  \usepackage[caption=false]{subfig}
  \usepackage{graphicx}
  \usepackage{todonotes}
  \usepackage{tikz,tikz-cd,pgf}
\usepackage{listings}
\usepackage[numbered]{mcode} %for matlab code
  
  \usepackage{algorithm}
  \usepackage{algpseudocode}
  
  \usepackage{pgfplots}
  \pgfplotsset{compat=1.12}
  
  \usepackage{multirow}
  \newcommand{\R}{\mathbb{R}}
  \newcommand{\N}[1]{\mathbb{N}^{#1}}
  \newcommand{\1}[1]{\mathds{1}_{#1}}
  \newcommand{\st}{\qquad\text{s. t.}\qquad}

  \renewcommand{\liminf}[1]{\underset{#1}{\lim\inf}\;}
  \renewcommand{\limsup}[1]{\underset{#1}{\lim\sup}\;}
  
  \newcommand{\CX}{\mathcal{X}}
  \newcommand{\CY}{\mathcal{Y}}
  \newcommand{\CZ}{\mathcal{Z}}
  \newcommand{\dif}{\mathrm{d}}
  
  \DeclareMathOperator*{\argmin}{\arg \min}%
  \DeclareMathOperator*{\argmax}{\arg \max}%
  \newcommand*\cpp{C\kern-0.2ex\raisebox{0.4ex}{\scalebox{0.8}{+\kern-0.4ex+}}}
  

  %For pseudocode
 \newcommand*\Let[2]{\State #1 $\gets$ #2}
  

\bibliographystyle{siam}

\title{A \textbf{\underline{Flex}}ible Primal-Dual Tool\textbf{\underline{Box}}}
\author{Hendrik Dirks}


\begin{document}
	
\maketitle
\section{Introduction}
Many variational problems can be written in the form
\begin{align}
	\argmin_{u} G(u) + F(Ku),
	%\argmin_{u=u_1,\ldots,u_n} \sum_{i=1}^{m_1} G_i(u) + \sum_{j=1}^{m_2} F_j(K_ju),
	\label{generalPrimalFormulation}
\end{align}
which is generally denoted as the \textit{primal} formulation of the minimization problem. It can be shown that minimizing \eqref{generalPrimalFormulation} is equivalent\todo{requirements} to minimizing the \textit{primal-dual} or \textit{saddle-point} formulation
\begin{align}
	\argmin_{u}\argmax_{p} G(u) + \langle y,Ku\rangle - F^*(y).
%\argmin_{\substack{u=u_1,\ldots,u_n\\p=p_1,\ldots,p_n}} \sum_{i=1}^{m_1} G_i(u) + \sum_{j=1}^{m_2} F_j(K_ju)
\label{generalPrimalDualFormulation}
\end{align}
It the recent years, efficient algorithms like ADMM \todo{ref} or primal-dual \todo{ref} for solving these saddle-point problems have become very popular. In this work we want to present an efficient toolbox that makes use the latter algorithm. The toolbox applies strategies like preconditioning to improve the convergence speed and has an optional \cpp backend to further reduce the runtime. From the user-side, the toolbox only requires to add the parts of the primal problem. Several operators are already implemented, but the toolbox is also capable of taking arbitrary operators $K$, as long as they have matrix form.

\section{Examples}

\subsection{Rudin-Osher-Fatemi}
The Rudin-Osher-Fatemi model model has very popular applications in image denoising. The primal-formulation reads
\begin{align}
	\argmin_u \frac{1}{2}\|u-f\|_2^2 + \alpha \|\nabla u\|_{1,2},
\end{align}
where the first part fits the unknown $u$ to the given input image $f$ and the second part refers to the so-called isotropic total variation, which penelized the total number jumps in the solution. Minimizing this problem with FlexBox can be done with the following lines of code

\begin{lstlisting} 
%Begin: Code example
main = flexbox;

numberU = main.addPrimalVar(size(f));

main.addTerm(L2dataTerm(0.5,f),numberU);
main.addTerm(L1gradientIso(0.08,size(f)),numberU);

main.runAlgorithm;

result = main.getPrimal(numberU);
%End: Code example
\end{lstlisting}
Let us begin in line $2$, which initializes a \textbf{FlexBox} object and saves it in the variable $main$. Line 4 then adds the primal objective variable $u$ which has the same size as the input image $f$. The toolbox returns the internal number of this primal variable, which is saved in $numberU$. \\
In line 6 and 7, the L$^2$-data-term with weight $0.5$ and corresponding image $f$ is added, moreover the isotropic TV-term with weight $0.08$ is pushed into the framework. Please mention that the function $addTerm$ always requires a functional part and the internal number of the corresponding primal variable. \\
The function call in line $9$ finally starts the calculation and once this is finished we transfer the solution into the variable $result$ in line $11$. 

\subsection{Optical Flow}

\subsection{Labeling}

\section{How-To}

\section{Arbitrary Operators}
\textbf{Classes}: \textit{L1operatorIso, L1operatorAniso, L2operator, frobeniusOperator}\\
Adding an arbitrary operator in some norm is simple. Let us assume we some term $\alpha \|Ku\|_{1,2}$ with 
\begin{align*}
	K=\begin{pmatrix}K_1&K_2\\K_3&K_4\end{pmatrix}, u=\begin{pmatrix}u_1\\u_2\end{pmatrix},
\end{align*}
where the norm $\|\cdot\|_{1,2}$ refers to the isotropic L1-norm. We can insert this term into \textbf{FlexBox} by calling
\begin{lstlisting} 
%Begin: Code example
main.addTerm(L1operatorIso(alpha,2,{K_1,K_2,K_3,K_4}),[numberU_1,numberU_2]);
%End: Code example
\end{lstlisting}
The operator $K$ has to be specified row-wise in a cell-array. The second argument $2$ tells the toolbox that every two elements in the cell-array correspond to one row. Please note that empty blocks in $K$ have to be specified as empty sparse matrices.

\section{Prox-Calculation}

\subsection{Frobenius-Norm}
Minimization problem
\begin{align}
	\min_u \alpha\|Ku\|_F	
\end{align}
Decuopling the operator $K$ reads
\begin{align}
	F(\tilde{u}) = \alpha\|\tilde{u}\|_F
\end{align}
Calculating convex conjugate (Frobenius-norm is self-adjoint)
\begin{align}
	F^*(y) = \delta_{\{ \|y/\alpha\|_{F^*} \leq 1 \}} = \delta_{\{ \|y/\alpha\|_{F} \leq 1 \}}
\end{align}
Prox reads:
\begin{align}
	prox_{\sigma F^*}(\tilde{y}) = \min_y \|y-\tilde{y}\|_2^2 + \delta_{\{ \|y/\alpha\|_{F} \leq 1 \}}	
\end{align}
Some calculation yields the compact solution
\begin{align}
	prox_{\sigma F^*}(\tilde{y}) = \frac{\tilde{y}}{\max\{1, \|\tilde{y}/\alpha\|_{F} \}}
\end{align}


\subsection{Kullback-Divergence}
\todo{Calculate with factor alpha}
Minimization problem
\begin{align}
\min_u Ku-f+f\log\frac{f}{Ku} + \delta_{\{ \cdot\geq 0 \}}(Ku)	
\end{align}
Decuopling the operator reads
\begin{align}
	F(\tilde{u}) = \tilde{u}-f+f\log\frac{f}{\tilde{u}} + \delta_{\{ \cdot\geq 0 \}}(\tilde{u})	
\end{align}
Calculating convex conjugate
\begin{align}
F^*(y) = -f\log(\boldsymbol{1}-y) + \delta_{\{ y\geq 0 \}}(\boldsymbol{1}-y)	
\end{align}
Prox reads:
\begin{align}
prox_{\sigma F^*}(\tilde{y}) = \min_y \frac{1}{2}\|y-\tilde{y}\|_2^2 -\sigma f\log(\boldsymbol{1}-y) + \delta_{\{ y\geq 0 \}}(\boldsymbol{1}-y)
\end{align}
Some calculation yields the compact solution
\begin{align}
prox_{\sigma F^*}(\tilde{y}) = \frac{1}{2}\left( \boldsymbol{1} + \tilde{y} - \sqrt{ (\tilde{y}-\boldsymbol{1})^2 + 4\sigma f }\right) 
\end{align}

\subsection{Inner Products}
\begin{align}
	\min_u \alpha\langle Ku,b\rangle
\end{align}
Decoupling the operator reads
\begin{align}
	F(\tilde{u}) = \langle\tilde{u},\alpha b\rangle
\end{align}
Calculating convex conjugate
\begin{align}
	F^*(y) = \sup_{\tilde{u}} \langle\tilde{u},y\rangle -\langle\tilde{u},\alpha b\rangle = \sup_{\tilde{u}} \langle\tilde{u},y - \alpha b\rangle = \begin{cases}0&\text{for } y=\alpha b\\\infty&\text{else}\end{cases}
\end{align}
Prox reads:
\begin{align}
prox_{\sigma F^*}(\tilde{y}) = \min_y \frac{1}{2}\|y-\tilde{y}\|_2^2 +\sigma \delta_{\{ y = \alpha b \}}
\end{align}
Compact solution
\begin{align}
prox_{\sigma F^*}(\tilde{y}) = \alpha b
\end{align}
\listoftodos

%\bibliographystyle{plain}
%\bibliography{references}
	
\end{document}